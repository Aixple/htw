\section{Klassifikation von Mustern}
\slides{WS2016_Kap1_Kap2}{13}
\slides{WS2016_Kap1_Kap2}{14}
\begin{center}
\fbox{Eingabedaten ($\RR^n$)} $\to$ NN $\to$ \fbox{(gewünschte) Ausgabedaten ($\RR^m$)}
\end{center}
NN: Abbildungsfunktion… typischer Weise \emph{nicht} als geschlossenen mathematischen Ausdruck gegeben, sondern durch Beispiele $\Rightarrow$ Stützstellen
\unimptnt{\slides{WS2016_Kap1_Kap2}{15}}
\subsection{Struktur eines allgemeinen Mustererkennungsproblems}
\slides{WS2016_Kap1_Kap2}{16}

\section{Funktionsapproximation}
\slides{WS2016_Kap1_Kap2}{17}
\slides{WS2016_Kap1_Kap2}{18}
\slides{WS2016_Kap1_Kap2}{19}

\unimptnt{
\section{Musterrekonstruktion / -vervollständigung}
\slides{WS2016_Kap1_Kap2}{20}
}
\section{Zusammenfassung}
\slides{WS2016_Kap1_Kap2}{21}
\subsection{Generalisierung}
Netzwerk soll nicht Daten auswendig lernen (und damit nur in einer festen Umwelt funktioniert), sondern generalisieren.
\slides{WS2016_Kap1_Kap2}{22}
$\Rightarrow$ mit dem Training dann aufhören, wenn $E_V$ und $E_L$ gleichzeitig möglichst klein sind.
\slides{WS2016_Kap1_Kap2}{23}
Überanpassung, da approximierte Abbildungsfunktion auf allen Punkten der Beispieldaten liegt.



